---
title: Uses of {gnm} for Generalized (Non)linear Modelling
author: Heather Turner
affiliation: University of Warwick
date: "2025-11-26"
date-format: long
footer: <https://hturner.github.io/BIBC>
format:
  warwickpres-revealjs: default
---

## Background

```{r}
#| label: setup
#| echo: false
library(dplyr)
library(knitr)
library(gnm)
library(foreign)
library(survival)
library(tictoc)
```


:::: columns
::: {.column width="60%"}
{gnm} (Turner and Firth, 2007) was released on CRAN 20 years ago ðŸŽ‰

::: fragment
Time to 

* Review the original motivation for {gnm}

* Explore ways {gnm} has been used in practice
:::
:::

::: {.column width="40%"}
![Photo by <a href="https://unsplash.com/@mpadb">Shaylyn</a> on <a href="https://unsplash.com/photos/a-three-tiered-cake-with-candles-sticking-out-of-it-2O4va26lBEI">Unsplash</a>](images/shaylyn-2O4va26lBEI-unsplash.jpg){fig-alt="Three tier cake with pink and white icing, sprinkles and multicolour candles on top" height=500}
:::
::::

:::{.notes}
When I was preparing for this talk, I realised than gnm is 20 years old!
:::

# Introduction to GNMs and {gnm} {.lavendar-bg}

## Generalized Nonlinear Models (GNMs)

In a Generalized *Linear* Model (GLM) we have

$$
g(E(y_i)) = g(\mu_i) = \beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi}
$$

and 

$$
\text{Var}(y_i) = \phi a_i V(\mu_i)
$$

::: fragment
A GNM extends this to 
$$
g(\mu_i) = \eta(\boldsymbol{x}_i; \boldsymbol{\beta})
$$
where $\eta(\boldsymbol{x}_i; \boldsymbol{\beta})$ is nonlinear in the parameters $\boldsymbol{\beta}$.
:::

:::{.notes}
Should I use $\phi a V(\mu)$ here?
:::

## Motivation

  GNMs may be thought of as...

::: fragment
  ... an extension of Nonlinear Least Squares

  * using a link function to model a 
  non-Gaussian response
:::

::: fragment
  ... an extension of GLMs

   * using nonlinear functions of parameters to produce a more parsimonious
  model and interpretable model.
:::

::: {.fragment .center-h .larger125}
**{gnm} focuses on the latter**
:::

## Classic Models that are GNMs

 - Goodman's RC model for 2 way tables (Goodman, 1979, *JASA*) 
    $$\log \mu_{rc} = \alpha_r + \beta_c + \gamma_r\delta_c$$
 - Stereotype model for ordered categorical data (Anderson, 1984, *JRSSB*)
    $$pr(y_i = c | \boldsymbol{x}_i) = \frac{\exp(\beta_{0c} + \gamma_c \boldsymbol{\beta}^T \boldsymbol{x}_i)}
    {\sum_r\exp(\beta_{0r} + \gamma_r \boldsymbol{\beta}^T \boldsymbol{x}_i)}$$
 - Lee-Carter age-specific mortality model (Lee and Carter, 1992, *JASA*)
 $$\log(\mu_{ay}/e_{ay}) = \alpha_a + \beta_a \gamma_y$$

::: notes
Goodman's RC aka GAMMI models to analyse genotype-by-environment interactions 
(maybe more general ressponse, e.g. crop yield) 
 - Stereotype (medical pov, eliminate)
 - Lee Carter (age-specific mortality models)
Other, no need to highlight here: Diag Ref, UNIDIFF, sum of exponentials]
 - mind you, example of double exponential decay model: 
https://research-repository.uwa.edu.au/en/publications/pollen-mediated-gene-flow-from-glyphosate-resistant-common-waterh
:::

# Poisson models to analyse case-crossover studies {.blue-bg}

## Effect of Ozone Pollution on Deaths in London 

Armstrong et al (2014, *BMC Medical Research Methodology*) provide the 
following example data original from Bhaskaran et al, 2013,
*International Journal of Epidemiology*:

* `date` daily from 2002-01-01 to 2006-12-31
* `ozone` daily average, Âµg/m$^3$
* `temperature` $^\circ$C
* `numdeaths`

```{r}
library(foreign)
london <- read.dta("londondataset2002_2006.dta")
```

:::notes
Is there an association between day-to-day variation in ozone levels and daily risk of death? A regression approach will also allow control for multiple potential confounding factors.
exposure of interest: ozone
outcome: death (count)
potential confounders: temperature and relative humidity
Could add scatterplot here as Figure 1 of https://academic.oup.com/ije/article/42/4/1187/657875
Associations over short timescales more likely to represent real causal relationships
:::

:::notes
I think the model we are fitting is Option 1 in the time regressions paper. This is
equivalent to the case-control data
:::

## Case cross-over data

```{r}
#| label: london-prep
#| echo: false
# create strata
london <- london |> 
  mutate(month = format(date, format="%b"),
         year = format(date, format="%Y"),
         weekday = factor(weekdays(date, abbreviate = TRUE),
                      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", 
                                 "Sat", "Sun")),
         stratum = paste(year, month, weekday, sep = "-"),
         stratum = factor(stratum, unique(stratum)),
         record = seq_along(date))

# create case-control sets
case_control <- select(london, record, date, weekday, weight = numdeaths)
case_control <- tapply(case_control, london$stratum,
                       expandCategorical, catvar = "date", countvar = "case",
                       group = FALSE)
case_control <- bind_rows(case_control) |>
  select(-id)
rownames(case_control) <- NULL
# add covariates matching dates
id <- match(case_control$date, london$date)
london_expanded <- cbind(case_control, 
                         select(london, ozone, 
                                temperature, relative_humidity)[id,])
rownames(london_expanded) <- NULL
```

Define month-weekday strata:

::: {.smaller90}
```{r}
#| label: london-slice
#| echo: false
select(london, record, date, weekday, stratum, numdeaths, ozone, temperature) |>
  slice(1) |>
  kable(digits = 2)
```
:::

Expand each record to make case-control sets within the strata:

::: {.smaller90}
```{r}
#| label: london_-slice_expanded
#| echo: false
select(london_expanded, record, date, weekday, case, weight, ozone, temperature) |>
  slice(1:5) |>
  kable(digits = 2)
```
:::

:::{.notes}
The strata are used to control for slow or regular (e.g. day-of-week) changes in underlying risk which might confound associations with the exposure of interest
:::

## Conditional logistic model

Given that one case occurs in each stratum, we can model the event $D_{i,s}$ 
that the death in stratum $s$ occurs on day $i$ as follows:

$$D_{i,s} \sim \text{Bernoulli} \left(\pi_i = \frac{\exp \left\{\boldsymbol{\beta}^T\boldsymbol{x}_i\right\}}{\sum_{j \in s(i)} \exp \left\{ \boldsymbol{\beta}^T \boldsymbol{x}_j\right\}}\right)$$
In our London example, we have:

* 8034 rows: 1826 dates $\times$ 4-5 of each weekday per month
* 2 parameters: one for `ozone` and one for `temperature`

:::notes
P(i is the case in stratum s) = pi_i
:::

## Conditional logistic model in R {visibility="hidden"}

Because we have weights, need to fit as Cox proportional hazards model
```{r}
#| label: clogit
fit <- coxph(Surv(rep(1, nrow(london_expanded)), case) ~ ozone + temperature + strata(record),
             data = london_expanded,
             weights = london_expanded$weight)
```
::: notes
This is a bit of a distraction here
:::

## Equivalent Poisson model

Armstrong et al (2014) propose to fit the equivalent Poisson model to 
$y_{i,s}$, the number of deaths on date $i$ in stratum $s$, where

$$
E(y_i) = \exp(\alpha_s + \boldsymbol{\beta}^T\boldsymbol{x}_i)
$$
and $\alpha_s$ are nuisance parameters that ensure the multinomial denominators 
are matched.

In this case we have

* 1826 rows, one for each date
* 2 parameters of interest plus $5\times 12 \times 7 = 420$ nuisance parameters!

::: notes
Could give some pros here (adjusting for overdispersion and auto-correlation )
 Levy and others (2001) point out equivalence
 
 5 years, 12 months, 7 days
:::

## Fitting GLMs

GLMs are estimated with an IWLS algorithm, where

$$
         \hat{\boldsymbol{\beta}}^{(r + 1)} = \left(\boldsymbol{X}^{T}\boldsymbol{W}^{(r)}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{W}^{(r)}\boldsymbol{z}^{(r)}
$$

::: fragment
For GNMs, {gnm}: 

* Uses a **generalized inverse** to avoid specifying identifiability constraints 
for nonlinear terms.

* Can **eliminate** parameters of a nuisance factor from $\boldsymbol{X}$, for 
efficient estimation.
:::

::: {.notes}
X is the design matrix
W diag matrix of working weights
z is the working response (constructed from y, mu, eta and deta/dmu)
:::

## Augmented Least Squares

For an ordinary least squares model
$$
\begin{align*}
        \left[(\boldsymbol{y}|\boldsymbol{X})^T(\boldsymbol{y}|\boldsymbol{X})\right]^{-1} &=
        \begin{pmatrix}
            \boldsymbol{y}^T\boldsymbol{y} & \boldsymbol{y}^T\boldsymbol{X} \\
            \boldsymbol{X}^T\boldsymbol{y} & \boldsymbol{X}^T\boldsymbol{X} \\
        \end{pmatrix}^{-1} =
        \begin{pmatrix}
            \boldsymbol{A}_{11} & \boldsymbol{A}_{12} \\
            \boldsymbol{A}_{21} & \boldsymbol{A}_{22} \\
        \end{pmatrix}
    \end{align*}
$$
    where $\boldsymbol{A}_{11}, \boldsymbol{A}_{12}$ and $\boldsymbol{A}_{22}$ are functions of $\boldsymbol{y}^T\boldsymbol{y}$, $\boldsymbol{X}^T\boldsymbol{y}$ and $\boldsymbol{X}^T\boldsymbol{X}$.

:::fragment
It can be shown that
$$
    \begin{align*}
        \hat{\boldsymbol{\beta}} &= (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
        = - \frac{\boldsymbol{A}_{21}}{\boldsymbol{A}_{11}}
    \end{align*}
$$
requiring only the first row (column) of the inverse to be found.
:::

## Application to Nuisance Parameters I

Let
$$
\tilde{\boldsymbol{X}} = \boldsymbol{W}^{\frac{1}{2}}(\boldsymbol{z}|\boldsymbol{X}) =
(\boldsymbol{U} | \boldsymbol{V})
$$

 * $\boldsymbol{V}$ is the $nk \times n$ matrix of dummy variables corresponding 
 to the nuisance factor with $n$ levels.
 * $\boldsymbol{U}$ is a $nk \times (p + 1)$ matrix where $p$ is the number of
model parameters.

::: {.fragment .center-h}
<br>
**Typically $\boldsymbol{n} \boldsymbol{>>} \boldsymbol{p}$**
:::

## Application to Nuisance Parameters II

Then
$$
    \begin{align*}
        (\tilde{\boldsymbol{X}}^T\tilde{\boldsymbol{X}})^{-} &=
        \begin{pmatrix}
            \boldsymbol{U}^T\boldsymbol{U} & \boldsymbol{U}^T\boldsymbol{V} \\
            \boldsymbol{V}^T\boldsymbol{U} & \boldsymbol{V}^T\boldsymbol{V} \\
        \end{pmatrix}^{-}
        = \begin{pmatrix}
            \boldsymbol{B}_{11} & \boldsymbol{B}_{12} \\
            \boldsymbol{B}_{21} & \boldsymbol{B}_{22} \\
        \end{pmatrix}
    \end{align*}
$$
Again, only the first row (column) of this generalised inverse is required to
estimate $\hat{\boldsymbol{\beta}}$, so we are only interested in $\boldsymbol{B}_{11}$ and
    $\boldsymbol{B}_{12}$.
    
$$  
    \begin{align*}
        \boldsymbol{B}_{11} &= (\boldsymbol{U}^T\boldsymbol{U} -
        \boldsymbol{U}^T\boldsymbol{V}(\boldsymbol{V}^T\boldsymbol{V})^{-1}\boldsymbol{V}^T\boldsymbol{U})^{-} \\
        \boldsymbol{B}_{12} &= - (\boldsymbol{V}^T\boldsymbol{V})^{-1}\boldsymbol{V}^T\boldsymbol{U}\boldsymbol{B}_{11}
    \end{align*}
$$

## Elimination of the Nuisance Factor

The structure of $\tilde{\boldsymbol{X}}$ makes things simpler

* $\boldsymbol{U}^T\boldsymbol{U}$ is $(p + 1) \times (p + 1)$: not expensive to compute.
* $\boldsymbol{V}^T\boldsymbol{V}$ is $n \times n$ diagonal: the non-zero
elements can be computed directly.
* $\boldsymbol{V}^T\boldsymbol{U}$ is $n \times (p + 1)$: equivalent to 
aggregating the rows of $\boldsymbol{U}$ by levels of the nuisance factor.

::: {.fragment .center-h}
<br>
**So we do not need to construct the large $nk \times n$ matrix $\boldsymbol{V}$.**
:::

::: notes
We only need to construct the $nk \times (p + 1)$ matrix $\boldsymbol{U}$, 
saving memory and reducing the computational burden.
:::

## Application to the Case-Crossover Poisson Model

Fit with standard GLM function

```{r}
tic()
poisson_glm <- glm(numdeaths ~ ozone + temperature + stratum, 
                   data = london, family = poisson)
toc()
```

::: fragment
```{r}
tic()
poisson_gnm <- gnm(numdeaths ~ ozone + temperature, eliminate = stratum, 
                   data = london, family = poisson)
toc()
```
:::

::: fragment
```{r}
coef(poisson_gnm)
```
:::

## Explore further

Methodology and London example:

* Armstrong et al (2014) [Conditional Poisson models: a flexible alternative to conditional logistic case cross-over analysis](https://doi.org/10.1186/1471-2288-14-122), BMC Medical Research Methodology [~250 citations]

Some recent Australasian applications:

* Chen et al (2022) [Ambient air pollution and epileptic seizures: A panel study 
in Australia](https://doi.org/10.1111/epi.17253), *Epilepsia*

* Campbell et al (2024) [Assessing mortality associated with heatwaves in the cool 
climate region of Tasmania, Australia](https://doi.org/10.1016/j.joclim.2024.100302), 
*The Journal of Climate Change and Health*

# Models for longitudinal categorical data {.yellow-bg}

## Longitudinal categorical data

We have repeated measures of a categorical outcome for multiple subjects, e.g.

| Subject | Time 1 | Time 2     | Time 3     |
| ------- | ------ | ---------- | ---------- |
| 1       | None   | Mild       | Moderate   |
| 2       | None   | None       | Severe     |
| 3       | Mild   | Moderate   | Moderate   |

Let $y_{it} \in \left\{1, ..., J\right\}$ be the response for subject $i$ at time $t$.

We want to model the marginal probabilities

$$P(y_{it} = j | \boldsymbol{x}_{it})$$
regarding the correlation between repeated measures as a nuisance.

::: notes
Marginal (univariate) probabilities for one time point vs multivariate over all times jointly
On average, how do the probabilities change with covariates
:::

## Indicator vector representation

Represent the categorical response for subject $i$ at time $t$ as a vector of integers:

$$\boldsymbol{y}_{it} = (y_{i1}, ..., y_{iJ})^\top$$

where

$$y_{ij} =
\begin{cases}
1, & \text{if } y_i = j, \\
0, & \text{otherwise.}
\end{cases}$$
Then the expected value is the vector of category probabilities we want to model
$$E[\boldsymbol{y}_{it} | \boldsymbol{x}_{it}] = 
\left( P(y_{it} = 1 | \boldsymbol{x}_{it}),\, P(y_{it} = 2 | \boldsymbol{x}_{it}),\, \dots,\, P(y_{it} = J | \boldsymbol{x}_{it}) \right)^\top = \boldsymbol{\pi}_{it}$$

## Generalized Estimating Equation Approach

1. Model the marginal mean $E[\boldsymbol{y}_{it} | \boldsymbol{x}_{it}] = \boldsymbol{\pi}_{it}(\boldsymbol{\beta})$
2. Stack the repeated outcomes for a subject: 
$\boldsymbol{y}_i = (\boldsymbol{y}_{i1}^\top, ..., \boldsymbol{y}_{iT}^\top)^\top$
3. Specify a "working" covariance matrix: $\text{Var}(\boldsymbol{y}_i) = \boldsymbol{V}_i(\boldsymbol{\beta}, \hat{\boldsymbol{\alpha}})$
4. Solve the estimating equation
$$\boldsymbol{U(\boldsymbol{\beta}, \hat{\boldsymbol{\alpha}})}= \sum_i \boldsymbol{D}_i^{\top} \boldsymbol{V}_i^{-1} (\boldsymbol{y}_i - \boldsymbol{\pi}_i) = \boldsymbol{0}, 
\quad 
\mathbf{D}_i = \frac{\partial \boldsymbol{\pi}_i}{\partial \boldsymbol{\beta}^\top}$$

::: fragment
* **No need to specify the full joint distribution**
* **Gives consistent estimates of $\boldsymbol{\beta}$ even if $\boldsymbol{\alpha}$ mis-specified**
:::

## Marginal Model

A univariate multinomial model can provide the required model for $\boldsymbol{\pi}_{it}$, e.g.

* Baseline-category multinomial logit model for nominal outcomes

    $$\log \left(\frac{\pi_{itj}}{\pi_{itJ}}\right) = \beta_{j0} + \boldsymbol{\beta}_j^\top \mathbf{x}_{it}, 
\quad j = 1, \dots, J-1$$

* Cumulative link model for ordinal outcomes

    $$F^{-1}\bigl[P(y_{it} \le j \mid \mathbf{x}_{it})\bigr] = \beta_{j0} + \boldsymbol{\beta}^\top \mathbf{x}_{it}, 
\quad j = 1, \dots, J-1$$
    from which $\pi_{itj}$ can be derived as differences between cumulative probabilities.
    
::: notes
Also for ordinal data:
adjacent categories logit model: log odds of one category vs the next, with common coefficients across categories (vs category-specific coefficients)
For cumulative model can have logit, probit, cauchit, complementary-log-log, i.e. 
any of the link functions available for binomial GLMS in R
:::

## Modelling the Covariance Matrix $V_i$

Any model for $V_i$ will result in consistent estimates for $\boldsymbol{\beta}$, 
however, if the model is

* Too simple: lose efficiency
* Too complex: convergence problems, imprecise estimation of $\boldsymbol{\alpha}$

Want a parsimonious representation that captures the correlation patterns

:::fragment
Touloumis et al (2013, *Biometrics*) propose to use **association models** for the 
marginalized contingency tables, implemented in R package {multgee}.
:::

::: notes
efficient: uses the available information better, leading to smaller standard errors (overall) for the same amount of data.
:::

## Touloumis et al Approach

1. For each time pair $g = (t, t')$ create a 2-way contingency table of frequencies $f_{jj'g}$.
2. Model the 3-way contingency table with the homogeneous RC-G(1) model:
$$\log f_{jj'g} = \lambda + \lambda^R_j + \lambda^C_{j'} + \lambda^G_g + \lambda^{RG}_{jg} + \lambda^{CG}_{j'g} + \phi^g\mu^g_j\mu^g_{j'}$$
3. Define $\boldsymbol{\alpha}$ as the vector of all local odds ratios, $\theta_{jj'g}$, based on
$$\left[\begin{matrix}
f_{jj'g} & f_{j(j'+ 1)g} \\
f_{(j+ 1)j'g} & f_{(j + 1)(j'+ 1)g}
\end{matrix}\right]$$
i.e. $\log(\theta_{jj'g}) = \phi^g (\mu^g_j - \mu^g_{j+1}) (\mu^g_{j'} - \mu^g_{j'+1})$
4. Estimate $P(y_{it} = j, y_{it'} = j'|x_i)$ by iterative proportional fitting to calculate $V_i$

::: notes
t' = "t prime"
:::

## Alternative Association Models

Different models for $\log(\theta_{jj'g})$ can be obtained through different 
models for $\phi^g\mu^g_j\mu^g_{j'}$:

::: smaller80
| Structure             | Model for Log Local Odds Ratio                   | Interaction Formula  | Response |
|-----------------------|---------------------------------------------|----------------------------------|----------|
| Independent           | 1                                                            | N/A                   | Any      |
| Uniform               | $\phi$                                                       | `~ r:c`                 | Ordinal  |
| Category exchangeable | $\phi^g$                                                     | `~ G:r:c`               | Ordinal  |
| Time exchangeable     | $\phi(\mu_j-\mu_{j+1})(\mu_{j'}-\mu_{j'+1})$           | `~ MultHomog(R,C)`      | Any      |
| Time-specific RC      | $\phi^g (\mu^g_j-\mu^g_{j+1})(\mu^g_{j'}-\mu^g_{j'+1})$ | `~ MultHomog(G:R, G:C)` | Any      |
:::

Where `MultHomog()` is a function in {gnm} for specifying homogeneous 
multiplicative interactions ($\mu_r\mu_c$).

:::{.notes}
| Time-specific RC      | $\phi^g (\mu^g_j - \mu^g_{j+1}) (\mu^g_{j'} - \mu^g_{j'+1})$ | `~ MultHomog(G:R, G:C)     ` | Any      |
:::

## Example: Housing data

The `housing` data from {multgee} is an example of nominal longitudinal data:

* `y`: housing in streets or shelters (0), community housing (1), or independent housing (2)
* `time`: 0, 6, 12, and 24 months
* `sec`: binary indicator of access to Section 8 Rental Certificate (financial support)

```{r}
library(multgee)
data(housing)
head(housing, 4)
```

:::{.notes}
nominal data
:::

## Modelling with Local Odds Ratios GEE

Estimate time-specific intrinsic parameters $\phi^g$ (in a RC-G(1) model):

```{r}
#| include: false
phi <- intrinsic.pars(y, data = housing, id = id, repeated = time, 
                      rscale = "nominal")
```

```{r}
#| eval: false
phi <- intrinsic.pars(y, data = housing, id = id, repeated = time, rscale = "nominal")
phi
```

```{r}
#| echo: false
phi
```

There is a wide range in the strength of association, so should use time-specific RC model

```{r}
fit_exch <- nomLORgee(y ~ factor(time) * sec, data = housing, id = id, repeated = time)
fit_rc <- update(fit_exch, LORstr = "RC")
fit_ind <- update(fit_exch, LORstr = "independence")
gee_criteria(fit_ind, fit_exch, fit_rc)[,1:3]
```
::: notes
a lower QIC tells you that overall, the time-exchangeable structure captures the dependence pattern better and produces a more efficient model.
:::

## Exploring further

The housing data is analysed in: 

* Touloumis, A. (2011) [GEE for multinomial responses](https://ufdcimages.uflib.ufl.edu/UF/E0/04/32/26/00001/touloumis_a.pdf). PhD dissertation, University of Florida.

The model is described in:

* Touloumis, A. et al (2013) [GEE for multinomial responses using a local odds ratios parameterization](https://doi.org/10.1111/biom.12054) *Biometrics* [~150 citations]

A recent Australasian example:

* Cooper, IL et al (2025) [Returning to work following parental leave: the experiences of Australian anaesthetists](https://doi.org/10.1177/0310057X241265726), Anaesthesia and Intensive Care.

# Modelling the mean-variance relationship for in-vitro diagnostic assays {.coral-bg}

## Performance of In-Vitro Diagnostic (IVD) Assays

Manufacturers of IVD assays need to provide information the precision of their product.

This is based on a precision experiment. `CA19_9` from {VCA} is an example from the Clinical and Laboratory Standards Institute guidelines.

6 samples were assayed with 5 replicates on 5 days, at 3 sites

* `result`: measured concentration of CA19-9 (tumour marker)
* `day`: day 1, 2, 3, 4, or 5
* `site`: laboratory 1, 2 or 3
* `sample`: P1, P2, P5, Q3, Q4, Q6 ("P" patient sample pool, "Q" control, number indicates concentration)

::: notes
Carbohydrate antigen 19-9 (CA 19-9) is a protein in the blood. It is a tumour marker used to monitor cances of the pancreas or bile ducts

All 3 sites used same samples: prepared at site 1 then distributed to other sites
https://www.normsplash.com/Samples/CLSI/151253346/CLSI-EP05-A3-2014-en-2.pdf
:::

## Variance Component Analysis

`anovaVCA()` from {VCA} computes variance components based on ANOVA:

```{r}
library(VCA)
data(CA19_9)
anovaVCA(result ~ site/day, Data = subset(CA19_9, sample == "P1"))
```

:::notes
yes really, capital D
:::

## Precision Performance Table 

Doing this for all samples, we can obtain a "Precision Performance Table"

|           | Mean   | Reproducibility %CV | Between-Site %CV | Between-Day %CV | Repeatability %CV |
|-----------|--------|---------------------|------------------|-----------------|-------------------|
| sample.P1 | 12.08  | 8.6                 | 5.1              | 3.5             | 6.0               |
| sample.P2 | 41.58  | 4.4                 | 3.1              | 0.8             | 3.1               |
| sample.Q3 | 55.75  | 4.1                 | 3.2              | 1.3             | 2.2               |
| sample.Q4 | 165.70 | 3.8                 | 3.3              | 0.8             | 1.7               |
| sample.P5 | 379.10 | 2.4                 | 1.3              | 0.5             | 2.0               |
| sample.Q6 | 414.30 | 3.7                 | 3.1              | 0.4             | 2.1               |

where *Reproducibility* is the total variation and *Repeatability* is the pure assay imprecision.

## Precision Profiles

Precision profiles model the relationship between the (component of) variance 
and the mean response. 

{VFP} implements several models as in the Variance Function Program software:

| Number | Model                                                   | Type      |
|--------|---------------------------------------------------------|-----------|
| 1      | $\sigma^2 = 1$                                       | linear    |
| 6      | $\sigma^2 = \beta_1 + \beta_2 \mu +   \beta_3 \mu^J$ | nonlinear |
| 7      | $\sigma^2 = \beta_1 + \beta_2 \mu^J$                 | nonlinear |
| 8      | $\sigma^2 = (\beta_1 + \beta_2 \mu)^J$               | nonlinear |
| 9      | $\sigma^2 = \beta_1 \mu^J$                           | nonlinear |

plus special cases Models 2-5 with $J$ set to 2 or another specified value.

## GNMs for Precision Profiles

For balanced designs we have 

$$E(\hat{\sigma}_c^2) = \sigma_c^2 \qquad \text{Var}(\hat{\sigma}_c^2) \approx \frac{2}{\nu_c}\sigma_c^4$$

where $\nu_c$ is the degrees of freedom for $\hat{\sigma}_c^2$. 

Therefore we can fit precision profiles using a Gamma GNM with an identity link

$$E(\hat{\sigma}_c^2) = \eta(\mu, \boldsymbol{\beta}) \qquad \text{Var}(\hat{\sigma}_c^2) = \frac{\phi}{\alpha_c} E(\hat{\sigma}_c^2)^2$$

with weights $\alpha_c = \nu_c/2$.

## Modelling Total Variability

First use `anovaVCA()` to estimate variance components for each sample:
```{r}
vca <- anovaVCA(result ~ site/day, Data = CA19_9, by = "sample")
```

Then use `get_mat()` to get the mean, a total variance (`VC`), corresponding degrees of freedom (`DF`) 
and the coefficient of variation (as %, `CV`) for all samples:
```{r}
library(VFP)
total <- get_mat(vca, vc = "total")
total
```

:::{.notes}
CV = sd/mean * 100
can also set `vc` to `site`, `site:day` or `error`
:::

## Example: Model 7

```{r}
powfun7 <- function (x) {
    list(predictors = list(beta1 = 1, beta2 = 1, J = 1), 
         variables = list(substitute(x)), 
         term = function(predictors, variables) {
            paste0(predictors[1], "+", 
                   predictors[2], "*", variables[1], "^", predictors[3])
         })
}
class(powfun7) <- "nonlin"
mod7 <- gnm(VC ~ powfun7(Mean) - 1, family = Gamma(link = "identity"), weights = DF/2, 
                data = total, start = c(1, 1, 2), verbose = FALSE)
coef(mod7)
deviance(mod7)
```
## Selecting Variance Function

`fit_vfp()` will fit all 9 models and select best fitting by AIC

```{r}
#| eval: false
all_mod <- fit_vfp(total, quiet = TRUE)
all_mod
```
```{r}
#| include: false
all_mod <- fit_vfp(total, quiet = TRUE)
```

```{r}
#| echo: false
all_mod
```

## Functional sensitivity

Determine concentrations at which a specified CV is not exceeded.

```{r}
#| fig.width: 6
#| fig.height: 4
plot(all_mod, model.no = 4, type = "cv", ylim = c(0, 8), Prediction = 5)
```


:::notes
When you combine multiple random effects (each with its own variance estimate and DF) into a single total variance (the sum of variance components), the sampling distribution of that sum does not have an integer DF.

To account for this, VCA uses the Satterthwaite approximation to compute the effect DF for the total variance
:::

## Explore Further

The [VFP vignette](https://CRAN.R-project.org/package=VFP/vignettes/VFP_package_vignette.html) gives further detail on the use of precision profiles.

{VFP} aims to implement the functionality of the [Variance Function Program](https://www.aacb.asn.au/AACB/Resources/Variance-Function-Program.aspx) distributed by the Australasian Association for Clinical Biochemistry and Laboratory Medicine

* The [VFP documentation](https://www.aacb.asn.au/common/Uploaded%20files/aacb/members/tools/VFP200.pdf) by W. A. Sadler (formerly Christchurch hospital) gives several references relating to the methods and application.

## Summary

{gnm} provides a unified framework for a wide range of models

Applications "in the wild" have used several of its features:

* The "eliminate" feature for efficient handling of nuisance parameters
* In-built "nonlin" functions to specify multiplicative terms
* Custom "nonlin" functions for specialized models

For more on {gnm} see the [vignette](https://cran.r-project.org/package=gnm/vignettes/gnmOverview.pdf) or the [BIBC 2025 tutorial](https://github.com/hturner/gnm-day-course)

## References

Turner, H and D Firth (2007) [gnm: A Package for Generalized Nonlinear Models](https://journal.r-project.org/articles/RN-2007-012/) *R News*

Anderson, J. A. (1984). [Regression and Ordered Categorical Variables](https://doi.org/10.1111/j.2517-6161.1984.tb01270.x) *JRSS B*

Goodman, L. A. (1979). [Simple models for the analysis of association in
cross-classifications having ordered categories](https://doi.org/10.1080/01621459.1979.10481650) *JASA*

Lee, R. D. and L. Carter (1992). [Modelling and forecasting the time series of
{US} mortality](https://doi.org/10.1080/01621459.1992.10475265) *JASA*