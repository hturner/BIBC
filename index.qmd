---
title: Uses of gnm for Generalized (Non)linear Modelling
author: Heather Turner
affiliation: University of Warwick
date: "2025-11-26"
date-format: long
footer: <https://hturner.github.io/BIBC>
format:
  warwickpres-revealjs: default
---

## Background

```{r}
#| label: setup
#| echo: false
library(dplyr)
library(knitr)
library(gnm)
library(foreign)
library(survival)
library(tictoc)
```


:::: columns
::: {.column width="60%"}
{gnm} was released on CRAN 20 years ago ðŸŽ‰

::: fragment
Time to 

* Review the original motivation for {gnm}

* Explore ways {gnm} has been used in practice
:::
:::

::: {.column width="40%"}
![Photo by <a href="https://unsplash.com/@mpadb">Shaylyn</a> on <a href="https://unsplash.com/photos/a-three-tiered-cake-with-candles-sticking-out-of-it-2O4va26lBEI">Unsplash</a>](images/shaylyn-2O4va26lBEI-unsplash.jpg){fig-alt="Three tier cake with pink and white icing, sprinkles and multicolour candles on top" height=500}
:::
::::

:::{.notes}
When I was preparing for this talk, I realised than gnm is 20 years old!
:::

# Introduction to GNMs and {gnm}

## Generalized Nonlinear Models (GNMs)

In a Generalized *Linear* Model (GLM) we have

$$
g(E(y_i)) = g(\mu_i) = \beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi}
$$

and 

$$
\text{Var}(y_i) = \phi a_i V(\mu_i)
$$

::: fragment
A GNM extends this to 
$$
g(\mu_i) = \eta(\boldsymbol{x}_i; \boldsymbol{\beta})
$$
where $\eta(\boldsymbol{x}; \boldsymbol{\beta})$ is nonlinear in the parameters $\boldsymbol{\beta}$.
:::

:::{.notes}
Should I use $\phi a V(\mu)$ here?
:::

## Motivation

  GNMs may be thought of as...

::: fragment
  ... an extension of Nonlinear Least Squares

  * using a link function to model a 
  non-Gaussian response
:::

::: fragment
  ... an extension of GLMs

   * using nonlinear functions of parameters to produce a more parsimonious
  model and interpretable model.
:::

## Classic Models that are GNMs

 - Goodman's RC model for 2 way tables (JASA, 74, 1979) 
    $$\log \mu_{rc} = \alpha_r + \beta_c + \gamma_r\delta_c$$
 - Stereotype model for ordered categorical data (JRSSB, 46, 1984)
    $$pr(y_i = c | \boldsymbol{x}_i) = \frac{\exp(\beta_{0c} + \gamma_c \boldsymbol{\beta}^T \boldsymbol{x}_i)}
    {\sum_r\exp(\beta_{0r} + \gamma_r \boldsymbol{\beta}^T \boldsymbol{x}_i)}$$
 - Lee-Carter age-specific mortality model (JASA, 87, 1992)
 $$\log(\mu_{ay}/e_{ay}) = \alpha_a + \beta_a \gamma_y$$

::: notes
Goodman's RC aka GAMMI models to analyse genotype-by-environment interactions 
(maybe more general ressponse, e.g. crop yield) 
 - Stereotype (medical pov, eliminate)
 - Lee Carter (age-specific mortality models)
Other, no need to highlight here: Diag Ref, UNIDIFF, sum of exponentials]
 - mind you, example of double exponential decay model: 
https://research-repository.uwa.edu.au/en/publications/pollen-mediated-gene-flow-from-glyphosate-resistant-common-waterh
:::

# Poisson models to analyse case-crossover studies in epidemiology {.blue-bg}

## Example: Effect of Ozone Pollution on Deaths in London 

Example data (Bhaskaran et al, International Journal of Epidemiology 42, 2013)

* `date` daily from 2002-01-01 to 2006-12-31
* `ozone` daily average, Âµg/m$^3$
* `temperature` $^\circ$C
* `relative_humidity` %
* `numdeaths`

```{r}
library(foreign)
london <- read.dta("londondataset2002_2006.dta")
```

:::notes
Is there an association between day-to-day variation in ozone levels and daily risk of death? A regression approach will also allow control for multiple potential confounding factors.
exposure of interest: ozone
outcome: death (count)
potential confounders: temperature and relative humidity
Could add scatterplot here as Figure 1 of https://academic.oup.com/ije/article/42/4/1187/657875
Associations over short timescales more likely to represent real causal relationships
:::

:::notes
I think the model we are fitting is Option 1 in the time regressions paper. This is
equivalent to the case-control data
:::

## Case cross-over data

```{r}
#| label: london-prep
#| echo: false
# create strata
london <- london |> 
  mutate(month = format(date, format="%b"),
         year = format(date, format="%Y"),
         weekday = factor(weekdays(date, abbreviate = TRUE),
                      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", 
                                 "Sat", "Sun")),
         stratum = paste(year, month, weekday, sep = "-"),
         stratum = factor(stratum, unique(stratum)),
         record = seq_along(date))

# create case-control sets
case_control <- select(london, record, date, weekday, weight = numdeaths)
case_control <- tapply(case_control, london$stratum,
                       expandCategorical, catvar = "date", countvar = "case",
                       group = FALSE)
case_control <- bind_rows(case_control) |>
  select(-id)
rownames(case_control) <- NULL
# add covariates matching dates
id <- match(case_control$date, london$date)
london_expanded <- cbind(case_control, 
                         select(london, ozone, 
                                temperature, relative_humidity)[id,])
rownames(london_expanded) <- NULL
```

Given records per day (temperature and relative humidity not shown)

::: {.smaller90}
```{r}
#| label: london-slice
#| echo: false
select(london, record, date, stratum, numdeaths, ozone) |>
  slice(1:2) |>
  kable(digits = 2)
```
:::

Expand to make case-control sets within month-weekday strata:

::: {.smaller90}
```{r}
#| label: london_-slice_expanded
#| echo: false
select(london_expanded, record, date, weekday, case, weight, ozone) |>
  slice(1:5) |>
  kable(digits = 2)
```
:::

## Conditional logistic model

Given that one case occurs in each stratum, we can model the event $D_{i,s}$ 
that the death in stratum $s$ occurs on day $i$ as follows:

$$D_{i,s} \sim \text{Bernoulli} \left(\pi_i = \frac{\exp \left\{\boldsymbol{\beta}^T\boldsymbol{x}_i\right\}}{\sum_{j \in s(i)} \exp \left\{ \boldsymbol{\beta}^T \boldsymbol{x}_j\right\}}\right)$$
In our London example, we have:

* 8034 rows: 1826 dates $\times$ 4-5 of each weekday per month
* 2 parameters: one for `ozone` and one for `temperature`

:::notes
P(i is the case in stratum s) = pi_i
:::

## Conditional logistic model in R {visibility="hidden"}

Because we have weights, need to fit as Cox proportional hazards model
```{r}
#| label: clogit
fit <- coxph(Surv(rep(1, nrow(london_expanded)), case) ~ ozone + temperature + strata(record),
             data = london_expanded,
             weights = london_expanded$weight)
```
::: notes
This is a bit of a distraction here
:::

## Equivalent Poisson model

Equivalently, we can fit a Poisson model to $y_{i,s}$, the number of deaths on 
date $i$ in stratum $s$, where

$$
E(y_i) = \exp(\alpha_s + \boldsymbol{\beta}^T\boldsymbol{x}_i)
$$
In this case we have

* 1826 rows, one for each date
* 2 parameters of interest plus $5\times 12 \times 7 = 420$ nuisance parameters!

::: notes
Could give some pros here (adjusting for overdispersion and auto-correlation )
 Levy and others (2001) point out equivalence
:::

## Fitting GLMs

GLMs are estimated with an IWLS algorithm, where

$$
         \boldsymbol{\beta}^{(r + 1)} = \left(\boldsymbol{X}^{T}\boldsymbol{W}^{(r)}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{W}^{(r)}\boldsymbol{z}^{(r)}
$$

{gnm} uses the same approach for GNMs, with a generalized inverse to avoid 
specifying identifiability constraints.

{gnm} also has an "eliminate" feature, to efficient estimate parameters of a 
nuisance factor, as in the Poisson model for case-crossover data.

## Augmented Least Squares

For an ordinary least squares model
$$
\begin{align*}
        \left[(\boldsymbol{y}|\boldsymbol{X})^T(\boldsymbol{y}|\boldsymbol{X})\right]^{-1} &=
        \begin{pmatrix}
            \boldsymbol{y}^T\boldsymbol{y} & \boldsymbol{y}^T\boldsymbol{X} \\
            \boldsymbol{X}^T\boldsymbol{y} & \boldsymbol{X}^T\boldsymbol{X} \\
        \end{pmatrix}^{-1} =
        \begin{pmatrix}
            \boldsymbol{A}_{11} & \boldsymbol{A}_{12} \\
            \boldsymbol{A}_{21} & \boldsymbol{A}_{22} \\
        \end{pmatrix}
    \end{align*}
$$
    where $\boldsymbol{A}_{11}, \boldsymbol{A}_{12}$ and $\boldsymbol{A}_{22}$ are functions of $\boldsymbol{y}^T\boldsymbol{y}$, $\boldsymbol{X}^T\boldsymbol{y}$ and $\boldsymbol{X}^T\boldsymbol{X}$.

Then it can be shown that
$$
    \begin{align*}
        \hat{\boldsymbol{\beta}} &= (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
        = - \frac{\boldsymbol{A}_{21}}{\boldsymbol{A}_{11}}
    \end{align*}
$$
requiring only the first row (column) of the inverse to be found.

## Application to Nuisance Parameters I

Let
$$
\tilde{\boldsymbol{X}} = \boldsymbol{W}^{\frac{1}{2}}(\boldsymbol{z}|\boldsymbol{X}) =
(\boldsymbol{U} | \boldsymbol{V})
$$

 * $\boldsymbol{V}$ is the $nk \times n$ matrix of dummy variables corresponding 
 to the nuisance factor with $n$ levels.
 * $\boldsymbol{U}$ is a $nk \times (p + 1)$ matrix where $p$ is the number of
model parameters, typically with $n >> p$.

## Application to Nuisance Parameters II

Then
$$
    \begin{align*}
        (\tilde{\boldsymbol{X}}^T\tilde{\boldsymbol{X}})^{-} &=
        \begin{pmatrix}
            \boldsymbol{U}^T\boldsymbol{U} & \boldsymbol{U}^T\boldsymbol{V} \\
            \boldsymbol{V}^T\boldsymbol{U} & \boldsymbol{V}^T\boldsymbol{V} \\
        \end{pmatrix}^{-}
        = \begin{pmatrix}
            \boldsymbol{B}_{11} & \boldsymbol{B}_{12} \\
            \boldsymbol{B}_{21} & \boldsymbol{B}_{22} \\
        \end{pmatrix}
    \end{align*}
$$
Again, only the first row (column) of this generalised inverse is required to
estimate $\hat{\boldsymbol{\beta}}$, so we are only interested in $\boldsymbol{B}_{11}$ and
    $\boldsymbol{B}_{12}$.
    
$$  
    \begin{align*}
        \boldsymbol{B}_{11} &= (\boldsymbol{U}^T\boldsymbol{U} -
        \boldsymbol{U}^T\boldsymbol{V}(\boldsymbol{V}^T\boldsymbol{V})^{-1}\boldsymbol{V}^T\boldsymbol{U})^{-} \\
        \boldsymbol{B}_{12} &= - (\boldsymbol{V}^T\boldsymbol{V})^{-1}\boldsymbol{V}^T\boldsymbol{U}\boldsymbol{B}_{11}
    \end{align*}
$$

## Elimination of the Nuisance Factor

* $\boldsymbol{U}^T\boldsymbol{U}$ is $p \times p$: not expensive to compute.
* $\boldsymbol{V}^T\boldsymbol{V}$ is diagonal: the non-zero elements can be computed
        directly
* $\boldsymbol{V}^T\boldsymbol{U}$ is equivalent to aggregating the rows of $\boldsymbol{U}$ by levels of the nuisance factor

So we do not need to construc the large $nk \times n$ matrix $\bm{V}$. 

We only need to construct the $nk \times (p + 1)$ matrix $\boldsymbol{U}$, 
saving memory and reducing the computational burden.

## Example

Fit with standard GLM function

```{r}
tic()
poisson_glm <- glm(numdeaths ~ ozone + temperature + stratum, 
                   data = london, family = poisson)
toc()
```

```{r}
tic()
poisson_gnm <- gnm(numdeaths ~ ozone + temperature, eliminate = stratum, 
                   data = london, family = poisson)
toc()
coef(poisson_gnm)
```

# Models for longitudinal categorical data {.yellow-bg}

## Longitudinal categorical data

We have repeated measures of a categorical outcome for multiple subjects, e.g.

| Subject | Time 1 | Time 2     | Time 3     |
| ------- | ------ | ---------- | ---------- |
| 1       | None   | Mild       | Moderate   |
| 2       | None   | None       | Severe     |
| 3       | Mild   | Moderate   | Moderate   |

Let $y_{it} \in \left\{1, ..., J\right\}$ be the response for subject $i$ at time $t$.

We want to model the marginal probabilities

$$P(y_{it} = j | \boldsymbol{x}_{it})$$
regarding the correlation between repeated measures as a nuisance.

::: notes
Marginal (univariate) probabilities for one time point vs multivariate over all times jointly
On average, how do the probabilities change with covariates
:::

## Indicator vector representation

Representing the categorical response $y_{it}$ as a vector of integers:

$$\boldsymbol{y}_{it} = (y_{i1}, ..., y_{iJ})^\top$$

where

$$y_{ij} =
\begin{cases}
1, & \text{if } y_i = j, \\
0, & \text{otherwise.}
\end{cases}$$
Then the expected value is the vector of category probabilities we want to model
$$E[\boldsymbol{y}_{it} | \boldsymbol{x}_{it}] = 
\left( P(y_{it} = 1 | \boldsymbol{x}_{it}),\, P(y_{it} = 2 | \boldsymbol{x}_{it}),\, \dots,\, P(y_{it} = J | \boldsymbol{x}_{it}) \right)^\top = \boldsymbol{\pi}_{it}$$

## Generalized Estimating Equation Approach

1. Model the marginal mean $E[\boldsymbol{y}_{it} | \boldsymbol{x}_{it}] = \boldsymbol{\pi}_{it}(\boldsymbol{\beta})$
2. Stack the repeated outcomes for a subject: 
$\boldsymbol{y}_i = (\boldsymbol{y}_{i1}^\top, ..., \boldsymbol{y}_{iT}^\top)^\top$
3. Specify a "working" covariance matrix: $\text{Var}(\boldsymbol{y}_i) = \boldsymbol{V}_i(\boldsymbol{\beta}, \boldsymbol{\alpha})$
4. Solve the estimating equation
$$\boldsymbol{U(\boldsymbol{\beta}, \boldsymbol{\alpha})}= \sum_i \\boldsymbolathbf{D}_i^{\top} \\boldsymbolathbf{V}_i^{-1} (\\boldsymbolathbf{y}_i - \boldsymbol{\pi}_i) = \boldsymbol{0}, 
\quad 
\mathbf{D}_i = \frac{\partial \boldsymbol{\pi}_i}{\partial \boldsymbol{\beta}^\top}$$

* No need to specify the full joint distribution
* Gives consistent estimates of $\boldsymbol{\beta}$ even if $\boldsymbol{\alpha}$ mis-specified

## Marginal Model

A univariate multinomial model can provide the required model for $\boldsymbol{\pi}_{it}$, e.g.

* Baseline-category multinomial logit model for nominal outcomes

    $$\log \left(\frac{\pi_{itj}}{\pi_{itJ}}\right) = \beta_{j0} + \boldsymbol{\beta}_j^\top \mathbf{x}_{it}, 
\quad j = 1, \dots, J-1$$

* Cumulative link model for ordinal outcomes

    $$F^{-1}\bigl[P(y_{it} \le j \mid \mathbf{x}_{it})\bigr] = \beta_{j0} + \boldsymbol{\beta}^\top \mathbf{x}_{it}, 
\quad j = 1, \dots, J-1$$
    from which $\pi_{itj}$ can be derived as differences between cumulative probabilities.
    
::: notes
Also for ordinal data:
adjacent categories logit model: log odds of one category vs the next, with common coefficients across categories (vs category-specific coefficients)
For cumulative model can have logit, probit, cauchit, complementary-log-log, i.e. 
any of the link functions available for binomial GLMS in R
:::

## Modelling the Covariance Matrix $V_i$

Any model for $V_i$ will result in consistent estimates for $\boldsymbol{\beta}$, 
however, if the model is

* Too simple: lose efficiency
* Too complex: convergence problems, imprecise estimation of $\boldsymbol{\alpha}$

Want a parsimonious representation that captures the correlation patterns

Touloumis et al (Biometrics, 69, 2013) propose to use association models for the 
marginalized contingnency tables, implemented in R package {multgee}.

::: notes
efficient: uses the available information better, leading to smaller standard errors (overall) for the same amount of data.
:::

## Touloumis et al Approach

1. For each time pair $g = (t, t')$ create a 2-way contingency table of frequencies $f_{jj'g}$.
2. Model the 3-way contingency table with the homogeneous RC-G(1) model:
$$\log f_{jj'g} = \lambda + \lambda^R_j + \lambda^C_{j'} + \lambda^G_g + \lambda^{RG}_{jg} + \lambda^{CG}_{j'g} + \phi^g\mu^g_j\mu^g_{j'}$$
3. Define $\boldsymbol{\alpha}$ as the vector of all local odds ratios, $\theta_{jj'g}$, based on
$$\left[\begin{matrix}
f_{jj'g} & f_{j(j'+ 1)g} \\
f_{(j+ 1)j'g} & f_{(j + 1)(j'+ 1)g}
\end{matrix}\right]$$
i.e. $\log(\theta_{jj'g}) = \phi^g (\mu^g_j - \mu^g_{j+1}) (\mu^g_{j'} - \mu^g_{j'+1})$
4. Estimate $P(y_{it} = j, y_{it'} = j'|x_i)$ by iterative proportional fitting to calculate $V_i$

::: notes
t' = "t prime"
:::

## Selecting the Association Model

The model for $\log(\theta_{jj'g})$ can be further simplified:

::: smaller80
| Structure             | Model for Log Local Odds Ratio                               | Association Model | Response |
|-----------------------|--------------------------------------------------------------|-------------------|----------|
| Independent           | 1                                                            | GLM               | Any      |
| Uniform               | $\phi$                                                       | GLM               | Ordinal  |
| Category exchangeable | $\phi^g$                                                     | GLM               | Ordinal  |
| Time exchangeable     | $\phi (\mu_j - \mu_{j+1}) (\mu_{j'} - \mu_{j'+1})$           | GNM               | Any      |
| Time-specific RC      | $\phi^g (\mu^g_j - \mu^g_{j+1}) (\mu^g_{j'} - \mu^g_{j'+1})$ | GNM               | Any      |
:::

We can select an adequate model by fitting the model with $\phi_g$ and using the simpler model if these parameters do not vary much.

## Example: Housing data

The `housing` data from {multgee} is an example of nominal longitudinal data:

* `y`: housing in streets or shelters (0), community housing (1), or independent housing (2)
* `time`: 0, 6, 12, and 24 months
* `sec`: binary indicator of access to Section 8 Rental Certificate (financial support)

```{r}
library(multgee)
data(housing)
head(housing, 4)
```

## Modelling with Local Odds Ratios GEE

Estimate time-specific intrinsic parameters:

```{r}
#| include: false
phi <- intrinsic.pars(y, data = housing, id = id, repeated = time, 
                      rscale = "nominal")
```

```{r}
#| eval: false
phi <- intrinsic.pars(y, data = housing, id = id, repeated = time, rscale = "nominal")
phi
```

```{r}
#| echo: false
phi
```

There is a wide range in the strength of association, so should use time-specific RC model

```{r}
fit_exch <- nomLORgee(y ~ factor(time) * sec, data = housing, id = id, repeated = time)
fit_rc <- update(fit_exch, LORstr = "RC")
fit_ind <- update(fit_exch, LORstr = "independence")
gee_criteria(fit_ind, fit_exch, fit_rc)[,1:3]
```
::: notes
a lower QIC tells you that overall, the time-exchangeable structure captures the dependence pattern better and produces a more efficient model.
:::

# Modelling the mean-variance relationship for in-vitro diagnostic assays {.coral-bg}

## Performance of In-Vitro Diagnostic (IVD) Assays

Manufacturers of IVD assays need to provide information the precision of their product.

This is based on a precision experiment. `CA19_9` from {VCA} is an example from the Clinical and Laboratory Standards Institute guidelines.

6 samples were assayed with 5 replicates on 5 days, at 3 sites

* `result`: measured concentration of CA19-9 (tumour marker)
* `day`: day 1, 2, 3, 4, or 5
* `site`: laboratory 1, 2 or 3
* `sample`: P1, P2, P5, Q3, Q4, Q6 ("P" patient sample pool, "Q" control, number indicates concentration)

::: notes
Carbohydrate antigen 19-9 (CA 19-9) is a protein in the blood. It is a tumour marker used to monitor cances of the pancreas or bile ducts

All 3 sites used same samples: prepared at site 1 then distributed to other sites
https://www.normsplash.com/Samples/CLSI/151253346/CLSI-EP05-A3-2014-en-2.pdf
:::

## Variance Component Analysis

`anovaVCA()` from {VCA} computes variance components based on ANOVA:

```{r}
library(VCA)
data(CA19_9)
anovaVCA(result ~ site/day, Data = subset(CA19_9, sample == "P1"))
```

:::notes
yes really, capital D
:::

## Precision Performance Table 

Doing this for all samples, we can obtain a "Precision Performance Table"

|           | Mean   | Reproducibility %CV | Between-Site %CV | Between-Day %CV | Repeatability %CV |
|-----------|--------|---------------------|------------------|-----------------|-------------------|
| sample.P1 | 12.08  | 8.6                 | 5.1              | 3.5             | 6.0               |
| sample.P2 | 41.58  | 4.4                 | 3.1              | 0.8             | 3.1               |
| sample.Q3 | 55.75  | 4.1                 | 3.2              | 1.3             | 2.2               |
| sample.Q4 | 165.70 | 3.8                 | 3.3              | 0.8             | 1.7               |
| sample.P5 | 379.10 | 2.4                 | 1.3              | 0.5             | 2.0               |
| sample.Q6 | 414.30 | 3.7                 | 3.1              | 0.4             | 2.1               |

where *Reproducibility* is the total variation and *Repeatability* is the pure assay imprecision.

## Precision Profiles

Precision profiles model the relationship between the (a component of) variance 
and the mean response. 

{VFP} implements several models as in the Variance Function Program software:

| Number | Model                                                   | Type      |
|--------|---------------------------------------------------------|-----------|
| 1      | $\sigma^2 = 1$                                       | linear    |
| 6      | $\sigma^2 = \beta_1 + \beta_2 \mu +   \beta_3 \mu^J$ | nonlinear |
| 7      | $\sigma^2 = \beta_1 + \beta_2 \mu^J$                 | nonlinear |
| 8      | $\sigma^2 = (\beta_1 + \beta_2 \mu)^J$               | nonlinear |
| 9      | $\sigma^2 = \beta_1 \mu^J$                           | nonlinear |

plus special cases Models 2-5 with $J$ set to 2 or another specified value.

## GNMs for Precision Profiles

For balanced designs we have 

$$E(\hat{\sigma}_c^2) = \sigma_c^2 \qquad \text{Var}(\hat{\sigma}_c^2) \approx \frac{2}{\nu_c}\sigma_c^4$$

where $\nu_c$ is the degrees of freedom for $\hat{\sigma}_c^2$. 

Therefore we can fit precision profiles using a Gamma GNM with an identity link

$$E(\hat{\sigma}_c^2) = \eta(\mu, \boldsymbol{\beta}) \qquad \text{Var}(\hat{\sigma}_c^2) = \frac{\phi}{\alpha_c} E(\hat{\sigma}_c^2)^2$$

with weights $\alpha_c = \nu_c/2$.

## Modelling Total Variability

First use `anovaVCA()` to estimate variance components for each sample:
```{r}
vca <- anovaVCA(result ~ site/day, Data = CA19_9, by = "sample")
```

Then use `get_mat()` to get the mean, a total variance (`VC`), corresponding degrees of freedom (`DF`) 
and the coefficient of variation (as %, `CV`) for all samples:
```{r}
library(VFP)
total <- get_mat(vca, vc = "total")
total
```

:::{.notes}
CV = sd/mean * 100
can also set `vc` to `site`, `site:day` or `error`
:::

## Example: Model 7

```{r}
powfun7 <- function (x) {
    list(predictors = list(beta1 = 1, beta2 = 1, J = 1), 
         variables = list(substitute(x)), 
         term = function(predictors, variables) {
            paste0(predictors[1], "+", 
                   predictors[2], "*", variables[1], "^", predictors[3])
         })
}
class(powfun7) <- "nonlin"
mod7 <- gnm(VC ~ powfun7(Mean) - 1, family = Gamma(link = "identity"), weights = DF/2, 
                data = total, start = c(1, 1, 2), verbose = FALSE)
coef(mod7)
deviance(mod7)
```
## Selecting Variance Function

`fit_vfp()` will fit all 9 models and select best fitting by AIC

```{r}
#| eval: false
all_mod <- fit_vfp(total, quiet = TRUE)
all_mod
```
```{r}
#| include: false
all_mod <- fit_vfp(total, quiet = TRUE)
```

```{r}
#| echo: false
all_mod
```

## Functional sensitivity

Determine concentrations at which a specified CV is not exceeded.

```{r}
#| fig.width: 6
#| fig.height: 4
plot(all_mod, model.no = 4, type = "cv", ylim = c(0, 8), Prediction = 5)
```


:::notes
When you combine multiple random effects (each with its own variance estimate and DF) into a single total variance (the sum of variance components), the sampling distribution of that sum does not have an integer DF.

To account for this, VCA uses the Satterthwaite approximation to compute the effect DF for the total variance
:::

## Summary

