---
title: Uses of gnm for Generalized (Non)linear Modelling
author: Heather Turner
affiliation: University of Warwick
date: "2025-11-26"
date-format: long
footer: <https://hturner.github.io/BIBC>
format:
  warwickpres-revealjs: default
---

## Background

```{r}
#| label: setup
#| echo: false
library(dplyr)
library(knitr)
library(gnm)
library(foreign)
library(survival)
library(tictoc)
```


:::: columns
::: {.column width="60%"}
{gnm} was released on CRAN 20 years ago ðŸŽ‰

::: fragment
Time to 

* Review the original motivation for {gnm}

* Explore ways {gnm} has been used in practice
:::
:::

::: {.column width="40%"}
![Photo by <a href="https://unsplash.com/@mpadb">Shaylyn</a> on <a href="https://unsplash.com/photos/a-three-tiered-cake-with-candles-sticking-out-of-it-2O4va26lBEI">Unsplash</a>](images/shaylyn-2O4va26lBEI-unsplash.jpg){fig-alt="Three tier cake with pink and white icing, sprinkles and multicolour candles on top" height=500}
:::
::::

:::{.notes}
When I was preparing for this talk, I realised than gnm is 20 years old!
:::

# Introduction to GNMs and {gnm}

## Generalized Nonlinear Models (GNMs)

In a Generalized *Linear* Model (GLM) we have

$$
g(E(y)) = \beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p}
$$

and 

$$
\text{Var}(y) = \phi V(\mu)
$$

::: fragment
A GNM extends this to 
$$
g(\mu) = \eta(\boldsymbol{x}; \bm{\beta})
$$
where $\eta(\bm{x}; \bm{\beta})$ is nonlinear in the parameters $\bm{\beta}$.
:::

:::{.notes}
Should I use $\phi a V(\mu)$ here?
:::

## Motivation

  GNMs may be thought of as...

::: fragment
  ... an extension of Nonlinear Least Squares

  * using a nonlinear function of a continuous variable to model a 
  non-Gaussian response
:::

::: fragment
  ... an extension of GLMs

   * using nonlinear functions of parameters to produce a more parsimonious
  model and interpretable model.
:::

## Models

::: notes
Some examples of models this framework covers:
 - Goodman's RC (RC(1), RC(2), Homog)
 - GAMMI (more from agri pov)
 - Stereotype (medical pov, eliminate)
 - Lee Carter (age-specific mortality models)
Other, no need to highlight here: Diag Ref, UNIDIFF, sum of exponentials]
 - mind you, example of double exponential decay model: 
https://research-repository.uwa.edu.au/en/publications/pollen-mediated-gene-flow-from-glyphosate-resistant-common-waterh
:::

# Conditional Poisson models to analyse case-crossover studies in epidemiology

## Example: Effect of Ozone Pollution on Deaths in London 

Example data (Bhaskaran et al, International Journal of Epidemiology 2013, 42: 1187-1195)

* `date` daily from 2002-01-01 to 2006-12-31
* `ozone` daily average, Âµg/m$^3$
* `temperature` $^\circ$C
* `relative_humidity` %
* `numdeaths`

```{r}
library(foreign)
london <- read.dta("londondataset2002_2006.dta")
```

:::notes
Is there an association between day-to-day variation in ozone levels and daily risk of death? A regression approach will also allow control for multiple potential confounding factors.
exposure of interest: ozone
outcome: death (count)
potential confounders: temperature and relative humidity
Could add scatterplot here as Figure 1 of https://academic.oup.com/ije/article/42/4/1187/657875
Associations over short timescales more likely to represent real causal relationships
:::

:::notes
I think the model we are fitting is Option 1 in the time regressions paper. This is
equivalent to the case-control data
:::

## Scatterplot

Need to look over short periods

## Case cross-over data

```{r}
#| label: london-prep
#| echo: false
# create strata
london <- london |> 
  mutate(month = format(date, format="%b"),
         year = format(date, format="%Y"),
         weekday = factor(weekdays(date, abbreviate = TRUE),
                      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", 
                                 "Sat", "Sun")),
         stratum = paste(year, month, weekday, sep = "-"),
         stratum = factor(stratum, unique(stratum)),
         record = seq_along(date))

# create case-control sets
case_control <- select(london, record, date, weekday, weight = numdeaths)
case_control <- tapply(case_control, london$stratum,
                       expandCategorical, catvar = "date", countvar = "case",
                       group = FALSE)
case_control <- bind_rows(case_control) |>
  select(-id)
rownames(case_control) <- NULL
# add covariates matching dates
id <- match(case_control$date, london$date)
london_expanded <- cbind(case_control, 
                         select(london, ozone, 
                                temperature, relative_humidity)[id,])
rownames(london_expanded) <- NULL
```

Given records per day (temperature and relative humidity not shown)

::: {.smaller90}
```{r}
#| label: london-slice
#| echo: false
select(london, record, date, stratum, numdeaths, ozone) |>
  slice(1:2) |>
  kable(digits = 2)
```
:::

Expand to make case-control sets within month-weekday strata:

::: {.smaller90}
```{r}
#| label: london_-slice_expanded
#| echo: false
select(london_expanded, record, date, weekday, case, weight, ozone) |>
  slice(1:5) |>
  kable(digits = 2)
```
:::

## Conditional logistic model

Given that one case occurs in each stratum, we can model the event $D_{i,s}$ 
that the death in stratum $s$ occurs on day $i$ as follows:

$$D_{i,s} \sim \text{Bernoulli} \left(\pi_i = \frac{\exp \left\{\beta^Tx_i\right\}}{\sum_{j \in s(i)} \exp \left\{ \beta^T x_j\right\}}\right)$$
In our London example, we have:

* 8034 rows: 1826 dates $\times$ 4-5 of each weekday per month
* 2 parameters: one for `ozone` and one for `temperature`

:::notes
P(i is the case in stratum s) = pi_i
:::

## Conditional logistic model in R

Because we have weights, need to fit as Cox proportional hazards model
```{r}
#| label: clogit
fit <- coxph(Surv(rep(1, nrow(london_expanded)), case) ~ ozone + temperature + strata(record),
             data = london_expanded,
             weights = london_expanded$weight)
```
::: notes
This is a bit of a distraction here
:::

## Equivalent Poisson model

Equivalently, we can fit a Poisson model to $Y_{i,s}$, the number of deaths on 
date $i$ in stratum $s$, where

$$
E(Y_i) = \exp(\alpha_s + \beta^Tx_i)
$$
In this case we have

* 1826 rows, one for each date
* 2 parameters of interest plus 1826 nuisance parameters!

::: notes
Could give some pros here (adjusting for overdispersion and auto-correlation )
 Levy and others (2001) point out equivalence
:::

## Eliminate feature



::: notes
Could go into depth about augmented least squares in gnm, or give reduced 
explanation about structure of design matrix and how we only need to compute on 
p x p matrix
:::

## Example

Fit with standard GLM function

```{r}
tic()
poisson_glm <- glm(numdeaths ~ ozone + temperature + stratum, 
                   data = london, family = poisson)
toc()
```

```{r}
tic()
poisson_gnm <- gnm(numdeaths ~ ozone + temperature, eliminate = stratum, 
                   data = london, family = poisson)
toc()
coef(poisson_gnm)
```

# Models for longitudinal categorical data

## Longitudinal categorical data

We have repeated measures of a categorical outcome for multiple subjects, e.g.

| Subject | Time 1 | Time 2     | Time 3     |
| ------- | ------ | ---------- | ---------- |
| 1       | None   | Mild       | Moderate   |
| 2       | None   | None       | Severe     |
| 3       | Mild   | Moderate   | Moderate   |

Let $Y_{it} \in \left\{1, ..., J\right\}$ be the response for subject $i$ at time $t$.

We want to model the marginal probabilities

$$P(Y_{it} = j | \boldsymbol{x}_{it})$$
Regarding the correlation between repeated measures for the same subject as a nuisance.

::: notes
Marginal (univariate) probabilities for one time point vs multivariate over all times jointly
On average, how do the probabilities change with covariates
:::

## Indicator vector representation

Representing the categorical response $Y_{it}$ as a vector of integers:

$$\boldsymbol{Y}_{it} = (Y_{i1}, ..., Y_{iJ})^\top$$

where

$$Y_{ij} =
\begin{cases}
1, & \text{if } Y_i = j, \\
0, & \text{otherwise.}
\end{cases}$$
Then the expected value is the vector of category probabilities we want to model
$$E[\boldsymbol{Y}_{it} | \boldsymbol{x}_{it}] = 
\left( P(Y_{it} = 1 | \boldsymbol{x}_{it}),\, P(Y_{it} = 2 | \boldsymbol{x}_{it}),\, \dots,\, P(Y_{it} = J | \boldsymbol{x}_{it}) \right)^\top = \boldsymbol{\pi}_{it}$$

## Generalized Estimating Equation Approach

1. Model the marginal mean $E[\boldsymbol{Y}_{it} | \boldsymbol{x}_{it}] = \boldsymbol{\pi}_{it}(\boldsymbol{\beta})$
2. Stack the repeated outcomes for a subject: 
$\boldsymbol{Y}_i = (\boldsymbol{Y}_{i1}^\top, ..., \boldsymbol{Y}_{iT}^\top)^\top$
3. Specify a "working" covariance matrix: $\text{Var}(\boldsymbol{Y}_i) = \boldsymbol{V}_i(\boldsymbol{\beta}, \boldsymbol{\alpha})$
4. Solve the estimating equation
$$\boldsymbol{U(\boldsymbol{\beta}, \boldsymbol{\alpha})}= \sum_i \mathbf{D}_i^{\top} \mathbf{V}_i^{-1} (\mathbf{Y}_i - \boldsymbol{\pi}_i) = \boldsymbol{0}, 
\quad 
\mathbf{D}_i = \frac{\partial \boldsymbol{\pi}_i}{\partial \boldsymbol{\beta}^\top}$$

* No need to specify the full joint distribution
* Gives consistent estimates of $\boldsymbol{\beta}$ even if $\boldsymbol{\alpha}$ mis-specified

## Marginal Model

A univariate multinomial model can provide the required model for $\boldsymbol{\pi}_{it}$, e.g.

* Baseline-category multinomial logit model for nominal outcomes

    $$\log \left(\frac{\pi_{itj}}{\pi_{itJ}}\right) = \beta_{j0} + \boldsymbol{\beta}_j^\top \mathbf{x}_{it}, 
\quad j = 1, \dots, J-1$$

* Cumulative link model for ordinal outcomes

    $$F^{-1}\bigl[P(Y_{it} \le j \mid \mathbf{x}_{it})\bigr] = \beta_{j0} + \boldsymbol{\beta}^\top \mathbf{x}_{it}, 
\quad j = 1, \dots, J-1$$
    from which $\pi_{itj}$ can be derived as differences between cumulative probabilities.
    
::: notes
Also for ordinal data:
adjacent categories logit model: log odds of one category vs the next, with common coefficients across categories (vs category-specific coefficients)
For cumulative model can have logit, probit, cauchit, complementary-log-log, i.e. 
any of the link functions available for binomial GLMS in R
:::

## Modelling the Covariance Matrix $V_i$

Any model for $V_i$ will result in consistent estimates for $\boldsymbol{\beta}$, 
however, if the model is

* Too simple: lose efficiency
* Too complex: convergence problems, imprecise estimation of $\boldsymbol{\alpha}$

Want a parsimonious representation that captures the correlation patterns

Touloumis et al (Biometrics, 69, 2013) propose to use association models for the 
marginalized contingnency tables, implemented in R package {multgee}.

::: notes
efficient: uses the available information better, leading to smaller standard errors (overall) for the same amount of data.
:::

## Touloumis et al Approach

1. For each time pair $g = (t, t')$ create a 2-way contingency table of frequencies $f_{jj'g}$.
2. Model the 3-way contingency table with the homogeneous RC-G(1) model:
$$\log f_{jj'g} = \lambda + \lambda^R_j + \lambda^C_{j'} + \lambda^G_g + \lambda^{RG}_{jg} + \lambda^{CG}_{j'g} + \phi^g\mu^g_j\mu^g_{j'}$$
3. Define $\boldsymbol{\alpha}$ as the vector of all local odds ratios, $\theta_{jj'g}$, based on
$$\left[\begin{matrix}
f_{jj'g} & f_{j(j'+ 1)g} \\
f_{(j+ 1)j'g} & f_{(j + 1)(j'+ 1)g}
\end{matrix}\right]$$
i.e. $\log(\theta_{jj'g}) = \phi^g (\mu^g_j - \mu^g_{j+1}) (\mu^g_{j'} - \mu^g_{j'+1})$
4. Estimate $P(Y_{it} = j, Y_{it'} = j'|x_i)$ by iterative proportional fitting to calculate $V_i$

::: notes
t' = "t prime"
:::

## Selecting the Association Model

The model for $\log(\theta_{jj'g})$ can be further simplified:

::: smaller80
| Structure             | Model for Log Local Odds Ratio                               | Model Type | Response |
|-----------------------|--------------------------------------------------------------|------------|----------|
| Independent           | 1                                                            | GLM        | Any      |
| Uniform               | $\phi$                                                       | GLM        | Ordinal  |
| Category exchangeable | $\phi^g$                                                     | GLM        | Ordinal  |
| Time exchangeable     | $\phi (\mu_j - \mu_{j+1}) (\mu_{j'} - \mu_{j'+1})$           | GNM        | Any      |
| Time-specific RC      | $\phi^g (\mu^g_j - \mu^g_{j+1}) (\mu^g_{j'} - \mu^g_{j'+1})$ | GNM        | Any      |
:::

We can select an adequate model by fitting the model with $\phi_g$ and using the simpler model if these parameters do not vary much.

## Example: Housing data

The `housing` data from {multgee} is an example of nominal longitudinal data:

* `y`: housing in streets or shelters (0), community housing (1), or independent housing (2)
* `time`: 0, 6, 12, and 24 months
* `sec`: binary indicator of access to Section 8 Rental Certificate (financial support)

```{r}
library(multgee)
data(housing)
head(housing, 4)
```

## Modelling with Local Odds Ratios GEE

Estimate time-specific intrinsic parameters:

```{r}
#| include: false
phi <- intrinsic.pars(y, data = housing, id = id, repeated = time, 
                      rscale = "nominal")
```

```{r}
#| eval: false
phi <- intrinsic.pars(y, data = housing, id = id, repeated = time, rscale = "nominal")
phi
```

```{r}
#| echo: false
phi
```

There is a wide range in the strength of association, so should use time-specific RC model

```{r}
fit_exch <- nomLORgee(y ~ factor(time) * sec, data = housing, id = id, repeated = time)
fit_rc <- update(fit_exch, LORstr = "RC")
fit_ind <- update(fit_exch, LORstr = "independence")
gee_criteria(fit_ind, fit_exch, fit_rc)[,1:3]
```
::: notes
a lower QIC tells you that overall, the time-exchangeable structure captures the dependence pattern better and produces a more efficient model.
:::

# Modelling the mean-variance relationship for in-vitro diagnostic assays

